from py_compile import main
import streamlit as st
from langchain_openai import ChatOpenAI
import os
from dotenv import load_dotenv
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import requests
from bs4 import BeautifulSoup
import re
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, temperature=0)

def crawl_product_page(url):
    # Implement web crawling logic to extract raw text from the product page  
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    paragraphs = soup.find_all('p')
    raw_text = "\n".join([para.get_text() for para in paragraphs])
    return raw_text

def preprocess_text(raw_text):
    # Implement text cleaning and chunking logic
    cleaned_text = re.sub(r'\s+', ' ', raw_text).strip()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = text_splitter.split_text(cleaned_text)
    return chunks

def store_in_vector_db(chunks, metadata):
    # Implement logic to generate embeddings and store them in a vector database 
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    vector_db = Chroma.from_texts(chunks, embeddings, metadatas=[metadata]*len(chunks), collection_name="products")
    return vector_db

def hybrid_search(query):
    # Implement hybrid search logic combining semantic similarity and metadata filtering    
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    vector_db = Chroma(collection_name="products", embedding_function=embeddings)
    results = vector_db.similarity_search(query, k=5)
    return results

def generate_response(llm, relevant_chunks, query):
    # Implement response generation logic using LLM
    prompt_template = """You are an AI assistant for product research. Based on the following product information, answer the query: {query}

    Product Information:
    {product_info}
    """

    product_info = "\n".join([chunk.page_content for chunk in relevant_chunks])
    prompt = PromptTemplate(input_variables=["query", "product_info"], template=prompt_template)
    llm_chain = LLMChain(llm=llm, prompt=prompt)
    response = llm_chain.run(query=query, product_info=product_info)
    return response

def extract_metadata(raw_text, url):
    # Simple metadata extraction example
    title_match = re.search(r'<title>(.*?)</title>', raw_text, re.IGNORECASE)
    title = title_match.group(1) if title_match else "No Title Found"
    price_match = re.search(r'\$([0-9]+(\.[0-9]{2})?)', raw_text)
    price = price_match.group(0) if price_match else "Price Not Found"
    metadata = {
        "url": url,
        "title": title,
        "price": price
    }
    return metadata

def main():
    st.title("AI-Powered Product Research RAG System")
    st.write("Input product page URLs to extract data and perform product research.")
    url_input = st.text_area("Enter product page URLs (one per line):")
    if st.button("Process URLs"):
        urls = url_input.splitlines()
        all_chunks = []
        for url in urls:
            st.write(f"Crawling URL: {url}")
            raw_text = crawl_product_page(url)
            st.write("Preprocessing text...")
            chunks = preprocess_text(raw_text)
            metadata = extract_metadata(raw_text, url)
            st.write("Storing in vector database...")
            store_in_vector_db(chunks, metadata)
            all_chunks.extend(chunks)
        st.success("All URLs processed and data stored.")
        query = st.text_input("Enter your product research query:")
        if st.button("Search"):
            st.write("Performing hybrid search...")
            relevant_chunks = hybrid_search(query)
            st.write("Generating response...")
            response = generate_response(llm, relevant_chunks, query)
            st.write("Response:")
            st.write(response)
            st.write("Generated by AI-Powered Product Research RAG System")

if __name__ == "__main__":
    main()






